{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import os\n",
    "#import fsspec, os, netrc, aiohttp,dask\n",
    "from satsearch import Search\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gdal\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse,fromstring\n",
    "from affine import Affine\n",
    "from pandas import to_datetime\n",
    "import time\n",
    "import jinja2 as jj2\n",
    "from rasterio.crs import CRS\n",
    "from tempfile import NamedTemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION CONFIGURATION\n",
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from getpass import getpass\n",
    "\n",
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(\"~/.netrc\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "    del netrcDir\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}.netrc | chmod og-rw {0}.netrc | echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n",
    "    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n",
    "    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n",
    "    del homeDir\n",
    "\n",
    "# Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n",
    "    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n",
    "    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n",
    "    del homeDir\n",
    "del urs, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_cred = requests.get('https://lpdaac.earthdata.nasa.gov/s3credentials').json()\n",
    "s3_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup GDAL Env for optimum performance\n",
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='EMPTY_DIR', \n",
    "           AWS_NO_SIGN_REQUEST='YES',\n",
    "           GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           GDAL_SWATH_SIZE='200000000',\n",
    "           VSI_CURL_CACHE_SIZE='200000000',\n",
    "           CPL_VSIL_CURL_ALLOWED_EXTENSIONS='TIF',\n",
    "           GDAL_HTTP_UNSAFESSL='YES',\n",
    "           GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "           GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n",
    "           AWS_REGION='us-west-2',\n",
    "           AWS_SECRET_ACCESS_KEY=s3_cred['secretAccessKey'],\n",
    "           AWS_ACCESS_KEY_ID=s3_cred['accessKeyId'],\n",
    "           AWS_SESSION_TOKEN=s3_cred['sessionToken'])\n",
    "\n",
    "\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({'distributed.dashboard.link':'http://localhost:8888/proxy/8787/status'})#'https://localhost:8787/status'})\n",
    "cluster = LocalCluster(threads_per_worker=2)\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://cmr.earthdata.nasa.gov/stac/LPCLOUD' \n",
    "collection = 'C1711924822-LPCLOUD' #HLS\n",
    "bbox=[-104.79107047,   40.78311181, -104.67687336,   40.87008987]\n",
    "dates = '2020-01-01/2021-01-10'\n",
    "\n",
    "cat = get_STAC_items(url,collection,dates,','.join(map(str, bbox)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to build metadata for Stac Catalog\n",
    "def open_hls_meta(stac_id):\n",
    "    var_url = urlopen('https://cmr.earthdata.nasa.gov/search/concepts/'+stac_id)\n",
    "    xmldoc = parse(var_url)\n",
    "    res={stac_id:{}}\n",
    "    for child in xmldoc.findall('.//AdditionalAttributes/AdditionalAttribute'):\n",
    "        if child.find('Name').text in ['ULX',\n",
    "                                       'ULY',\n",
    "                                       'NROWS',\n",
    "                                       'NCOLS',\n",
    "                                       'SPATIAL_RESOLUTION',\n",
    "                                       'HORIZONTAL_CS_CODE',\n",
    "                                       'SENSING_TIME',\n",
    "                                       'MGRS_TILE_ID',\n",
    "                                       'CLOUD_COVERAGE',\n",
    "                                       'FILLVALUE',\n",
    "                                       'QA_FILLVALUE',\n",
    "                                       'MEAN_SUN_AZIMUTH_ANGLE',\n",
    "                                       'MEAN_SUN_ZENITH_ANGLE',\n",
    "                                       'MEAN_VIEW_AZIMUTH_ANGLE',\n",
    "                                       'MEAN_VIEW_ZENITH_ANGLE',\n",
    "                                       'NBAR_SOLAR_ZENITH',\n",
    "                                       'IDENTIFIER_PRODUCT_DOI',\n",
    "                                       'IDENTIFIER_PRODUCT_DOI_AUTHORITY',\n",
    "                                       'REF_SCALE_FACTOR',\n",
    "                                       'ADD_OFFSET']:\n",
    "            res[stac_id][child.find('Name').text]=child.find('.//Value').text\n",
    "    return(res)\n",
    "\n",
    "#Convert STAC catalog to VRT file(s) (1 vrt file per band)\n",
    "#Outputs as single xarray dataset object (dims = x,y,t ; variables = bands)\n",
    "def build_xr(catalog,bands,):\n",
    "    #Retreive Metadata using threads - not cpu bound, so works well\n",
    "    l_meta={}\n",
    "    t1 = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(5) as executor:\n",
    "        futures = []\n",
    "        for stac_id in list(catalog):\n",
    "            futures.append(executor.submit(open_hls_meta, stac_id=stac_id))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            l_meta.update(future.result())\n",
    "        for stac_id in list(catalog):\n",
    "            for b in bands:\n",
    "                l_meta[stac_id][b+'_url'] = catalog[stac_id][b].urlpath\n",
    "    \n",
    "    t2 = time.time()\n",
    "    print('Get File Meta',t2-t1)\n",
    "    \n",
    "    #Setup template file for each raster. Use Jinja to quickly fill in metadata.\n",
    "    vrt_template = jj2.Template('''\n",
    "    <VRTDataset rasterXSize=\"{{rasterXSize}}\" rasterYSize=\"{{rasterYSize}}\">\n",
    "      <SRS>{{SRS}}</SRS>\n",
    "      <GeoTransform>{{GeoTransform}}</GeoTransform>\n",
    "      <VRTRasterBand dataType=\"{{dtype}}\" band=\"1\">\n",
    "        <NoDataValue>{{nodata}}</NoDataValue>\n",
    "        <Scale>{{scale}}</Scale>\n",
    "        <Metadata>\n",
    "          <MDI key=\"obs_date\">{{obs_date}}</MDI>\n",
    "        </Metadata>\n",
    "        <SimpleSource>\n",
    "          <SourceFilename relativeToVRT=\"1\">/vsicurl/{{SourceFilename}}</SourceFilename>\n",
    "          <SourceBand>1</SourceBand>\n",
    "          <SourceProperties RasterXSize=\"{{rasterXSize}}\" RasterYSize=\"{{rasterYSize}}\" DataType=\"{{dtype}}\" BlockXSize=\"1024\" BlockYSize=\"1024\" />\n",
    "          <SrcRect xOff=\"0\" yOff=\"0\" xSize=\"{{rasterXSize}}\" ySize=\"{{rasterYSize}}\" />\n",
    "          <DstRect xOff=\"0\" yOff=\"0\" xSize=\"{{rasterXSize}}\" ySize=\"{{rasterYSize}}\" />\n",
    "          <NODATA>{{nodata}}</NODATA>\n",
    "        </SimpleSource>\n",
    "      </VRTRasterBand>\n",
    "    </VRTDataset>\n",
    "    ''')\n",
    "    \n",
    "    #Enumerate the stac catalog (by band) and create vrt objects in a dictionary (l_vrt)\n",
    "    l_vrt={}\n",
    "    for i, b in enumerate(bands):\n",
    "        l_tmp = []\n",
    "        for k in l_meta.keys():\n",
    "            item = l_meta[k]\n",
    "            vrt = vrt_template.render(rasterXSize = int(item['NCOLS']),\n",
    "                                      rasterYSize = int(item['NROWS']),\n",
    "                                      SourceFilename = catalog[k][b].urlpath,\n",
    "                                      SRS=CRS.from_epsg(item['HORIZONTAL_CS_CODE'].split(':')[1]).wkt,\n",
    "                                      GeoTransform=item['ULX']+', '+item['SPATIAL_RESOLUTION']+', 0, '+item['ULY']+', 0, -'+item['SPATIAL_RESOLUTION'],\n",
    "                                      band=b,\n",
    "                                      obs_date=item['SENSING_TIME'],\n",
    "                                      dtype='int16',\n",
    "                                      nodata=item['FILLVALUE'],\n",
    "                                      scale=item['REF_SCALE_FACTOR'])\n",
    "            l_tmp.append(vrt)\n",
    "        l_vrt[b] = l_tmp\n",
    "    \n",
    "    t3 = time.time()\n",
    "    print('Build VRT',t3-t2)\n",
    "    \n",
    "    #Use GDAL to merge vrts from the same bands\n",
    "    vrt_files={}\n",
    "    for b in bands:\n",
    "        with NamedTemporaryFile() as tmpfile:\n",
    "            vrt_options = gdal.BuildVRTOptions(separate=True,bandList=[1])\n",
    "            my_vrt = gdal.BuildVRT(tmpfile.name, l_vrt[b], options=vrt_options)\n",
    "            my_vrt = None\n",
    "            f = tmpfile.read().decode(\"utf-8\")\n",
    "            vrt_files[b]=f\n",
    "    \n",
    "    t4 = time.time()\n",
    "    print('Build VRT',t4-t3)\n",
    "    \n",
    "    #Extract metadata and lazy-load a nd merge into single xarray object\n",
    "    if type(bands) == str:\n",
    "        bands = [bands]\n",
    "    b_l = []\n",
    "    for b in bands:\n",
    "        v = vrt_files[b]\n",
    "        xmldoc = fromstring(v)\n",
    "        dates = []\n",
    "        for child in xmldoc.findall('''.//VRTRasterBand/ComplexSource/SourceFilename'''):\n",
    "            dates.append(to_datetime(child.text[child.text.find('obs_date')+10:\n",
    "                                                child.text.find('obs_date')+36]).date())\n",
    "        ds_tmp = xr.open_rasterio(v,chunks={'band':1,\n",
    "                                            'x':'auto',\n",
    "                                            'y':'auto'}).to_dataset(name=b)\n",
    "        ds_tmp = ds_tmp.rename({'band':'t'})\n",
    "        ds_tmp['t'] = dates\n",
    "        b_l.append(ds_tmp)\n",
    "    \n",
    "    t5 = time.time()\n",
    "    print('Create/Merge Xarray',t5-t4)\n",
    "    \n",
    "    ds = xr.merge(b_l)\n",
    "    return(ds,vrt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "da,vrts = build_xr(cat,['B8A', 'B03', 'B04', 'B12', 'Fmask'])\n",
    "print('Completed IN:',time.time()-t1,'seconds')\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = da.rename(t='time')\n",
    "da['time'] = da['time'].astype('datetime64[ns]')\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hls_funcs.masks import mask_hls\n",
    "da_mask = mask_hls(da['Fmask'])\n",
    "da_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj\n",
    "utmProj = Proj(\"+proj=utm +zone=13U, +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n",
    "bbox_utm = utmProj([bbox[i] for i in [0, 2]], [bbox[i] for i in [3, 1]]) \n",
    "tuple(bbox_utm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sub = da.loc[dict(x=slice(*tuple(bbox_utm[0])), y=slice(*tuple(bbox_utm[1])))].where(da_mask == 0)\n",
    "da_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_stacked = da_sub.stack(z=('y', 'x')).chunk(dict(time=50, z=-1))\n",
    "da_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hls_funcs.predict import pred_bm\n",
    "import pickle\n",
    "bm_mod = pickle.load(open('src/models/CPER_HLS_to_VOR_biomass_model_lr_simp.pk', 'rb'))\n",
    "da_bm = pred_bm(da_stacked, bm_mod, dim='z')\n",
    "da_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_bm = da_bm.unstack('z').persist()\n",
    "da_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param\n",
    "import panel as pn\n",
    "import datetime as dt\n",
    "def load_map(date):\n",
    "    return da_bm.isel(time=date).hvplot(x='x',y='y',rasterize=True,tiles='EsriImagery', crs=ccrs.UTM(13),\n",
    "                         cmap='inferno', clim=(100, 1000))\n",
    "\n",
    "date_slider = pn.widgets.IntSlider(name='Date Slider',\n",
    "                                    start=0, end=len(da_bm.time), value=0)\n",
    "\n",
    "@pn.depends(date=date_slider.param.value)\n",
    "def load_map_cb(date):\n",
    "    return load_map(date)\n",
    "\n",
    "pn.Row(pn.WidgetBox('Select date', date_slider), load_map_cb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

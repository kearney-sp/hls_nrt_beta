{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import LocalCluster, Client\n",
    "import xarray as xr\n",
    "import dask\n",
    "import intake\n",
    "import os\n",
    "#import fsspec, os, netrc, aiohttp,dask\n",
    "from satsearch import Search\n",
    "import hvplot.pandas\n",
    "import hvplot.xarray\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gdal\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from urllib.request import urlopen\n",
    "from xml.etree.ElementTree import parse,fromstring\n",
    "from affine import Affine\n",
    "from pandas import to_datetime\n",
    "import time\n",
    "import jinja2 as jj2\n",
    "from rasterio.crs import CRS\n",
    "from tempfile import NamedTemporaryFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUTHENTICATION CONFIGURATION\n",
    "from netrc import netrc\n",
    "from subprocess import Popen\n",
    "from getpass import getpass\n",
    "\n",
    "urs = 'urs.earthdata.nasa.gov'    # Earthdata URL to call for authentication\n",
    "prompts = ['Enter NASA Earthdata Login Username \\n(or create an account at urs.earthdata.nasa.gov): ',\n",
    "           'Enter NASA Earthdata Login Password: ']\n",
    "\n",
    "# Determine if netrc file exists, and if so, if it includes NASA Earthdata Login Credentials\n",
    "try:\n",
    "    netrcDir = os.path.expanduser(\"~/.netrc\")\n",
    "    netrc(netrcDir).authenticators(urs)[0]\n",
    "    del netrcDir\n",
    "\n",
    "# Below, create a netrc file and prompt user for NASA Earthdata Login Username and Password\n",
    "except FileNotFoundError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('touch {0}.netrc | chmod og-rw {0}.netrc | echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n",
    "    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n",
    "    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n",
    "    del homeDir\n",
    "\n",
    "# Determine OS and edit netrc file if it exists but is not set up for NASA Earthdata Login\n",
    "except TypeError:\n",
    "    homeDir = os.path.expanduser(\"~\")\n",
    "    Popen('echo machine {1} >> {0}.netrc'.format(homeDir + os.sep, urs), shell=True)\n",
    "    Popen('echo login {} >> {}.netrc'.format(getpass(prompt=prompts[0]), homeDir + os.sep), shell=True)\n",
    "    Popen('echo password {} >> {}.netrc'.format(getpass(prompt=prompts[1]), homeDir + os.sep), shell=True)\n",
    "    del homeDir\n",
    "del urs, prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_cred = requests.get('https://lpdaac.earthdata.nasa.gov/s3credentials').json()\n",
    "s3_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup GDAL Env for optimum performance\n",
    "env = dict(GDAL_DISABLE_READDIR_ON_OPEN='YES', \n",
    "           #AWS_NO_SIGN_REQUEST='YES',\n",
    "           #GDAL_MAX_RAW_BLOCK_CACHE_SIZE='200000000',\n",
    "           #GDAL_SWATH_SIZE='200000000',\n",
    "           #VSI_CURL_CACHE_SIZE='200000000',\n",
    "           #CPL_VSIL_CURL_ALLOWED_EXTENSIONS='TIF',\n",
    "           #GDAL_HTTP_UNSAFESSL='YES',\n",
    "           GDAL_HTTP_COOKIEFILE=os.path.expanduser('~/cookies.txt'),\n",
    "           GDAL_HTTP_COOKIEJAR=os.path.expanduser('~/cookies.txt'),\n",
    "           AWS_REGION='us-west-2',\n",
    "           AWS_SECRET_ACCESS_KEY=s3_cred['secretAccessKey'],\n",
    "           AWS_ACCESS_KEY_ID=s3_cred['accessKeyId'],\n",
    "           AWS_SESSION_TOKEN=s3_cred['sessionToken'])\n",
    "\n",
    "\n",
    "os.environ.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_cred = requests.get('https://lpdaac.earthdata.nasa.gov/s3credentials').json()\n",
    "s3_cred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a LUT dict including the HLS product bands mapped to names\n",
    "lut = {'HLSS30': \n",
    "       {'COASTAL-AEROSOL':'B01', 'BLUE':'B02', 'GREEN':'B03', 'RED':'B04', \n",
    "        'RED-EDGE1':'B05', 'RED-EDGE2':'B06', 'RED-EDGE3':'B07', 'NIR-Broad':'B08', 'NIR1':'B8A', \n",
    "        'WATER-VAPOR':'B09', 'CIRRUS':'B10', 'SWIR1':'B11', 'SWIR2':'B12', 'FMASK':'Fmask'},\n",
    "       'HLSL30': \n",
    "       {'COASTAL-AEROSOL':'B01', 'BLUE':'B02', 'GREEN':'B03', 'RED':'B04', \n",
    "        'NIR1':'B05', 'SWIR1':'B06','SWIR2':'B07', \n",
    "        'CIRRUS':'B09', 'TIR1':'B10', 'TIR2':'B11', 'FMASK':'Fmask'}}\n",
    "\n",
    "# List of all available/acceptable band names\n",
    "all_bands = ['ALL', 'COASTAL-AEROSOL', 'BLUE', 'GREEN', 'RED', 'RED-EDGE1', 'RED-EDGE2', 'RED-EDGE3', \n",
    "             'NIR1', 'SWIR1', 'SWIR2', 'CIRRUS', 'TIR1', 'TIR2', 'WATER-VAPOR', 'FMASK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as r\n",
    "stac = 'https://cmr.earthdata.nasa.gov/stac/' # CMR-STAC API Endpoint\n",
    "stac_response = r.get(stac).json()            # Call the STAC API endpoint\n",
    "stac_lp = [s for s in stac_response['links'] if 'LP' in s['title']]  # Search for only LP-specific catalogs\n",
    "\n",
    "# LPCLOUD is the STAC catalog we will be using and exploring today\n",
    "lp_cloud = r.get([s for s in stac_lp if s['title'] == 'LPCLOUD'][0]['href']).json()\n",
    "lp_links = lp_cloud['links']\n",
    "lp_collections = [l['href'] for l in lp_links if l['rel'] == 'collections'][0]  # Set collections endpoint to variable\n",
    "collections_response = r.get(f\"{lp_collections}\").json()    \n",
    "collections = collections_response['collections']\n",
    "hls_collections = [c for c in collections if 'HLS' in c['title']]\n",
    "s30 = [h for h in hls_collections if h['short_name'] == 'HLSS30'][0]  # Grab HLSS30 collection\n",
    "s30_id = s30['id']\n",
    "l30 = [h for h in hls_collections if h['short_name'] == 'HLSL30'][0]  # Grab HLSL30 collection\n",
    "l30_id = l30['id']\n",
    "\n",
    "lp_search = [l['href'] for l in lp_links if l['rel'] == 'search'][0]  # Define the search endpoint\n",
    "lim = 100\n",
    "search_query = f\"{lp_search}?&limit={lim}\"    # Add in a limit parameter to retrieve 100 items at a time.\n",
    "\n",
    "bbox_num=[-104.79107047,   40.78311181, -104.67687336,   40.87008987]\n",
    "bbox = f'{bbox_num[0]},{bbox_num[1]},{bbox_num[2]},{bbox_num[3]}'  # Defined from ROI bounds\n",
    "search_query2 = f\"{search_query}&bbox={bbox}\"                                                  # Add bbox to query\n",
    "date_time = '2021-01-01/2021-01-31'  # Define start time period / end time period\n",
    "search_query3 = f\"{search_query2}&datetime={date_time}\"  # Add to query that already includes bbox\n",
    "\n",
    "# Search for the HLSS30 items of interest:\n",
    "search_query4 = f\"{search_query3}&collections={s30_id}\"\n",
    "s30_items = r.get(search_query4).json()['features']\n",
    "\n",
    "# Search for the HLSL30 items of interest:\n",
    "search_query5 = f\"{search_query3}&collections={l30_id}\"\n",
    "l30_items = r.get(search_query5).json()['features']\n",
    "\n",
    "# Combine the S30 ad L30 items:\n",
    "hls_items = s30_items + l30_items\n",
    "\n",
    "print(f\"{len(hls_items)} items found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = hls_items[0]\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def stac_to_xr(item, band):\n",
    "    if band not in all_bands:\n",
    "        sys.exit(f\"Band: {band} is not a valid input option.\" +\n",
    "                 \"Valid inputs are ALL, COASTAL-AEROSOL, BLUE, GREEN, RED, RED-EDGE1, RED-EDGE2, RED-EDGE3,\" + \n",
    "                 \" NIR1, SWIR1, SWIR2, CIRRUS, TIR1, TIR2, WATER-VAPOR, FMASK.\" + \n",
    "                 \"To request multiple layers, provide them in comma separated format with no spaces.\" +\n",
    "                 \"Unsure of the names for your bands?\" +\n",
    "                 \"--check out the README which contains a table of all bands and band names.\")\n",
    "    if item['collection'] == s30_id:\n",
    "        href = item['assets'][lut['HLSS30'][band]]['href']\n",
    "    if item['collection'] == l30_id:\n",
    "        href = item['assets'][lut['HLSL30'][band]]['href']\n",
    "    ds = xr.open_rasterio(href, chunks={'band':1,\n",
    "                                            'x':'auto',\n",
    "                                            'y':'auto'}).to_dataset(name=band)\n",
    "    ds = ds.rename({'band': 'time'})\n",
    "    ds['time'] = [item['properties']['datetime']]\n",
    "    ds['time'] = ds['time'].astype('datetime64[ns]')\n",
    "    #ds = ds.assign_coords(t = datetime.strptime(item['properties']['datetime'], '%Y-%m-%dT%H:%M:%S.%fZ'))\n",
    "    return ds\n",
    "\n",
    "def stac_to_ds(items, bands):\n",
    "    ds_list = []\n",
    "    for item in items:\n",
    "        ds_item_list = []\n",
    "        for band in bands:\n",
    "            ds_item_list.append(stac_to_xr(item, band))\n",
    "            ds_item = xr.merge(ds_item_list)\n",
    "        ds_list.append(ds_item)\n",
    "    ds_out = xr.concat(ds_list, 'time')\n",
    "    return ds_out\n",
    "\n",
    "da = stac_to_ds(hls_items, ['BLUE', 'GREEN', 'RED', 'NIR1', 'SWIR1', 'SWIR2', 'FMASK'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da = da.rename(t='time')\n",
    "da['time'] = da['time'].astype('datetime64[ns]')\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hls_funcs.masks import mask_hls\n",
    "da_mask = mask_hls(da['Fmask'])\n",
    "da_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyproj import Proj\n",
    "utmProj = Proj(\"+proj=utm +zone=13U, +north +ellps=WGS84 +datum=WGS84 +units=m +no_defs\")\n",
    "bbox_utm = utmProj([bbox[i] for i in [0, 2]], [bbox[i] for i in [3, 1]]) \n",
    "tuple(bbox_utm[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_sub = da.loc[dict(x=slice(*tuple(bbox_utm[0])), y=slice(*tuple(bbox_utm[1])))].where(da_mask == 0)\n",
    "da_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_stacked = da_sub.stack(z=('y', 'x')).chunk(dict(time=50, z=-1))\n",
    "da_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hls_funcs.predict import pred_bm\n",
    "import pickle\n",
    "bm_mod = pickle.load(open('src/models/CPER_HLS_to_VOR_biomass_model_lr_simp.pk', 'rb'))\n",
    "da_bm = pred_bm(da_stacked, bm_mod, dim='z')\n",
    "da_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_bm = da_bm.unstack('z').persist()\n",
    "da_bm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param\n",
    "import panel as pn\n",
    "import datetime as dt\n",
    "import cartopy.crs as ccrs\n",
    "import holoviews as hv\n",
    "def load_map(date):\n",
    "    return da_bm.isel(time=date).hvplot(x='x',y='y',rasterize=True,tiles='EsriImagery', crs=ccrs.UTM(13),\n",
    "                         cmap='inferno', clim=(100, 1000))\n",
    "\n",
    "date_slider = pn.widgets.IntSlider(name='Date Slider',\n",
    "                                    start=0, end=len(da_bm.time), value=0)\n",
    "\n",
    "@pn.depends(date=date_slider.param.value)\n",
    "def load_map_cb(date):\n",
    "    return load_map(date)\n",
    "\n",
    "pn.Row(pn.WidgetBox('Select date', date_slider), load_map_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.hls_funcs.predict import pred_cov\n",
    "ends_dict = {\n",
    "    'SD': {\n",
    "        'ndvi': 0.30,\n",
    "        'dfi': 16,\n",
    "        'bai_126': 155},\n",
    "    'GREEN': {\n",
    "        'ndvi': 0.55,\n",
    "        'dfi': 10,\n",
    "        'bai_126': 160},\n",
    "    'BARE': {\n",
    "        'ndvi': 0.10,\n",
    "        'dfi': 8,\n",
    "        'bai_126': 140}}\n",
    "da_cov = pred_cov(da_stacked, ends_dict, dim='z')\n",
    "da_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da_cov = da_cov.to_array(dim='type')\n",
    "da_cov = da_cov.where(da_cov < 1.0, 1.0)\n",
    "da_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import param\n",
    "import panel as pn\n",
    "import datetime as dt\n",
    "import cartopy.crs as ccrs\n",
    "import holoviews as hv\n",
    "def load_map(date): \n",
    "    return da_cov.isel(time=date).hvplot.rgb(x='x',y='y', bands='type', tiles='EsriImagery', crs=ccrs.UTM(13))\n",
    "\n",
    "date_slider = pn.widgets.IntSlider(name='Date Slider',\n",
    "                                    start=0, end=len(da_cov.time), value=0)\n",
    "\n",
    "@pn.depends(date=date_slider.param.value)\n",
    "def load_map_cb(date):\n",
    "    return load_map(date)\n",
    "\n",
    "pn.Row(pn.WidgetBox('Select date', date_slider), load_map_cb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
